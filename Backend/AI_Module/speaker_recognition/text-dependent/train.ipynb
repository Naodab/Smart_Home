{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Audio preprocessing parameters\n",
    "    SAMPLING_RATE = 16000\n",
    "    DURATION = 3.0  # seconds\n",
    "    SAMPLES_PER_TRACK = int(SAMPLING_RATE * DURATION)\n",
    "    \n",
    "    # MFCC parameters\n",
    "    N_MFCC = 13\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 40\n",
    "    \n",
    "    # Data augmentation parameters\n",
    "    NOISE_FACTOR = 0.1\n",
    "    VALID_SPLIT = 0.15\n",
    "    SHUFFLE_SEED = 43\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    # Model parameters\n",
    "    NUM_CLASSES = 4  # Update based on your speaker count\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 50\n",
    "\n",
    "config = Config()\n",
    "print(config.N_MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/TechCare/AppData/Local/Programs/Python/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def load_and_preprocess_audio(file_path, config):\n",
    "    \"\"\"Load and preprocess audio file.\"\"\"\n",
    "    try:\n",
    "        # Đọc file audio sử dụng librosa thay vì tf.io.read_file\n",
    "        audio, sr = librosa.load(file_path, sr=config.SAMPLING_RATE, mono=True)\n",
    "        \n",
    "        # Normalize audio\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        \n",
    "        # Remove silence\n",
    "        audio = remove_silence(audio)\n",
    "        \n",
    "        # Normalize volume\n",
    "        audio = normalize_volume(audio)\n",
    "        \n",
    "        # Ensure consistent length\n",
    "        if len(audio) > config.SAMPLES_PER_TRACK:\n",
    "            audio = audio[:config.SAMPLES_PER_TRACK]\n",
    "        else:\n",
    "            # Pad with zeros if audio is too short\n",
    "            padding = config.SAMPLES_PER_TRACK - len(audio)\n",
    "            audio = np.pad(audio, (0, padding))\n",
    "        \n",
    "        # Convert to tensor\n",
    "        audio = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "        \n",
    "        return audio\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        # Return a zero tensor of the correct shape as fallback\n",
    "        return tf.zeros(config.SAMPLES_PER_TRACK, dtype=tf.float32)\n",
    "\n",
    "def normalize_volume(audio, target_db=-20.0):\n",
    "    \"\"\"Normalize the volume of an audio signal to a target decibel level.\"\"\"\n",
    "    try:\n",
    "        current_db = librosa.amplitude_to_db(np.abs(audio))\n",
    "        adjustment_db = target_db - np.mean(current_db)\n",
    "        normalized_audio = audio * np.power(10, adjustment_db / 20.0)\n",
    "        return normalized_audio\n",
    "    except:\n",
    "        return audio\n",
    "\n",
    "def remove_silence(audio, threshold=0.01, frame_length=2048, hop_length=512):\n",
    "    \"\"\"Remove silence from an audio signal based on a threshold.\"\"\"\n",
    "    try:\n",
    "        non_silent_indices = np.where(np.abs(audio) > threshold)[0]\n",
    "        if len(non_silent_indices) == 0:\n",
    "            return audio  # Return original audio if all is silent\n",
    "        start_index = non_silent_indices[0]\n",
    "        end_index = non_silent_indices[-1] + 1\n",
    "        return audio[start_index:end_index]\n",
    "    except:\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_audio(file_path, config):\n",
    "    \"\"\"Load and preprocess audio file.\"\"\"\n",
    "    try:\n",
    "        # Load audio file using librosa\n",
    "        audio, _ = librosa.load(str(file_path), sr=config.SAMPLING_RATE)\n",
    "        \n",
    "        # Ensure consistent length\n",
    "        if len(audio) > config.SAMPLES_PER_TRACK:\n",
    "            audio = audio[:config.SAMPLES_PER_TRACK]\n",
    "        else:\n",
    "            # Pad with zeros if audio is too short\n",
    "            padding = config.SAMPLES_PER_TRACK - len(audio)\n",
    "            audio = np.pad(audio, (0, padding))\n",
    "            \n",
    "        return tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_dataset(data_dir, noise_dir, config, augment=True):\n",
    "    \"\"\"Create dataset from directory with noise augmentation.\"\"\"\n",
    "    # Get all audio files and their labels\n",
    "    audio_data = []\n",
    "    labels = []\n",
    "    speaker_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(Path(data_dir) / d)]\n",
    "    \n",
    "    # Load speaker audio files\n",
    "    for label, speaker in enumerate(speaker_dirs):\n",
    "        speaker_path = Path(data_dir) / speaker\n",
    "        for audio_file in speaker_path.glob('*.wav'):\n",
    "            audio = load_audio(audio_file, config)\n",
    "            if audio is not None:\n",
    "                audio_data.append(audio)\n",
    "                labels.append(label)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    audio_data = tf.stack(audio_data)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((audio_data, labels))\n",
    "    \n",
    "    # Load and preprocess audio\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (load_and_preprocess_audio(x, config), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Filter out failed loadings (zero tensors)\n",
    "    dataset = dataset.filter(\n",
    "        lambda x, y: tf.not_equal(tf.reduce_sum(tf.abs(x)), 0.0)\n",
    "    )\n",
    "    \n",
    "    if augment:\n",
    "        # Load noise files if available\n",
    "        if noise_dir and os.path.exists(noise_dir):\n",
    "            noise_paths = list(Path(noise_dir).glob('*.wav'))\n",
    "            if noise_paths:\n",
    "                noise_dataset = tf.data.Dataset.from_tensor_slices([str(p) for p in noise_paths])\n",
    "                noise_dataset = noise_dataset.map(\n",
    "                    lambda x: load_and_preprocess_audio(x, config),\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE\n",
    "                )\n",
    "                noise_dataset = noise_dataset.cache()\n",
    "            else:\n",
    "                noise_dataset = None\n",
    "        else:\n",
    "            noise_dataset = None\n",
    "            \n",
    "        # Apply augmentation\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (apply_augmentation(x, config, noise_dataset), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (extract_mfcc(x, config), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    return dataset, len(speaker_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def add_background_noise(audio, noise_dataset, snr_db=10):\n",
    "    \"\"\"Add background noise to audio with specific SNR.\"\"\"\n",
    "    if noise_dataset is None:\n",
    "        return audio\n",
    "        \n",
    "    # Get a random noise sample\n",
    "    noise = next(iter(noise_dataset.shuffle(1)))\n",
    "    \n",
    "    # Calculate signal and noise power\n",
    "    signal_power = tf.reduce_mean(tf.square(audio))\n",
    "    noise_power = tf.reduce_mean(tf.square(noise))\n",
    "    \n",
    "    # Calculate noise scaling factor for target SNR\n",
    "    snr = tf.pow(10.0, snr_db / 10.0)\n",
    "    scale = tf.sqrt(signal_power / (noise_power * snr))\n",
    "    \n",
    "    # Add scaled noise to audio\n",
    "    noisy_audio = audio + scale * noise\n",
    "    \n",
    "    # Normalize\n",
    "    return noisy_audio / tf.reduce_max(tf.abs(noisy_audio))\n",
    "\n",
    "def time_shift(audio, shift_max=0.1):\n",
    "    \"\"\"Apply random time shift.\"\"\"\n",
    "    # Convert to int32 explicitly and handle float multiplication\n",
    "    size = tf.cast(tf.shape(audio)[0], tf.float32)\n",
    "    shift_amt = tf.cast(size * shift_max, tf.int32)\n",
    "    \n",
    "    # Generate random shift within range\n",
    "    shift = tf.random.uniform(\n",
    "        [], \n",
    "        minval=-shift_amt,\n",
    "        maxval=shift_amt + 1,  # +1 because uniform excludes maxval\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "    \n",
    "    return tf.roll(audio, shift, axis=0)\n",
    "\n",
    "def apply_augmentation(audio, config, noise_dataset=None):\n",
    "    \"\"\"Apply all augmentation techniques.\"\"\"\n",
    "    # Randomly apply augmentations\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        audio = add_background_noise(audio, noise_dataset, snr_db=10)\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        audio = time_shift(audio)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def extract_mfcc(audio, config):\n",
    "    \"\"\"Extract MFCC features from audio signal.\"\"\"\n",
    "    \n",
    "    # Convert to spectrogram\n",
    "    stfts = tf.signal.stft(\n",
    "        audio,\n",
    "        frame_length=config.N_FFT,\n",
    "        frame_step=config.HOP_LENGTH,\n",
    "        fft_length=config.N_FFT\n",
    "    )\n",
    "    \n",
    "    # Convert to magnitude spectrogram\n",
    "    spectrograms = tf.abs(stfts)\n",
    "    \n",
    "    # Convert to mel spectrograms\n",
    "    num_spectrogram_bins = tf.shape(spectrograms)[-1]\n",
    "    mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins=config.N_MELS,\n",
    "        num_spectrogram_bins=num_spectrogram_bins,\n",
    "        sample_rate=config.SAMPLING_RATE,\n",
    "        lower_edge_hertz=0,\n",
    "        upper_edge_hertz=config.SAMPLING_RATE/2\n",
    "    )\n",
    "    \n",
    "    mel_spectrograms = tf.tensordot(spectrograms, mel_weight_matrix, 1)\n",
    "    \n",
    "    # Convert to log mel spectrograms\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    \n",
    "    # Calculate MFCCs using DCT\n",
    "    mfccs = tf.signal.dct(log_mel_spectrograms, type=2, n=config.N_MFCC)\n",
    "    \n",
    "    # Add channel dimension for CNN\n",
    "    mfccs = tf.expand_dims(mfccs, -1)\n",
    "    \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build CNN model for speaker recognition.\"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # First Conv Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Conv Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Conv Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "def train_model(train_dir, noise_dir, config):\n",
    "    \"\"\"Train the speaker recognition model.\"\"\"\n",
    "    # Create datasets\n",
    "    dataset, num_speakers = create_dataset(train_dir, noise_dir, config, augment=True)\n",
    "    config.NUM_CLASSES = num_speakers\n",
    "    \n",
    "    # Calculate dataset size\n",
    "    dataset_size = len(list(dataset))\n",
    "    val_size = int(dataset_size * config.VALID_SPLIT)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    # Shuffle and batch datasets\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size, seed=config.SHUFFLE_SEED)\n",
    "    train_ds = dataset.take(train_size).batch(config.BATCH_SIZE)\n",
    "    val_ds = dataset.skip(train_size).batch(config.BATCH_SIZE)\n",
    "    \n",
    "    # Prefetch for performance\n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Get input shape from first batch\n",
    "    for batch in train_ds.take(1):\n",
    "        input_shape = batch[0].shape[1:]\n",
    "        break\n",
    "    \n",
    "    # Build and compile model\n",
    "    model = build_model(input_shape, config.NUM_CLASSES)\n",
    "    model = compile_model(model, config.LEARNING_RATE)\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_model.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=config.EPOCHS,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set directories\n",
    "    base_dir = Path(\"/kaggle/input/dataset-text-dependent/datasets\")\n",
    "    train_dir = base_dir / \"train\"\n",
    "    noise_dir = base_dir / \"noise\"\n",
    "    \n",
    "    # Initialize config\n",
    "    # config = Config()\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(train_dir, noise_dir, config)\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(\"final_model.keras\")\n",
    "    \n",
    "    # Plot training history\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
